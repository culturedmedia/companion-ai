# CompanionAI ü¶ä

A voice-first personal assistant app with whimsical animal companions. Built with React Native + Expo.

## Features

### üé§ Voice-First Interaction
- Add tasks by speaking naturally: "Add task to call mom tomorrow"
- Query your tasks: "What do I need to do today?"
- Complete tasks: "Mark grocery shopping as done"
- Get focus suggestions: "What should I focus on?"

### üêæ Choose Your Companion
Pick from 8 unique whimsical animals, each with their own personality:
- ü¶ä **Fox** - Clever & Resourceful
- ü¶â **Owl** - Wise & Observant
- üê± **Cat** - Independent & Cozy
- üê∞ **Bunny** - Gentle & Encouraging
- üêâ **Dragon** - Powerful & Protective
- ü¶é **Axolotl** - Chill & Adaptable
- üêº **Red Panda** - Playful & Curious
- üêß **Penguin** - Loyal & Determined

### üìã Smart Task Management
- Auto-categorization based on keywords
- Priority levels (High, Medium, Low)
- Due dates and times
- Recurring tasks
- Calendar view

### üéÆ Gamification
- Earn coins for completing tasks
- Level up your companion
- Shop for cosmetics and boosts
- Unlock achievements
- Maintain streaks

### üí¨ Proactive Companion
Your companion asks questions and provides encouragement:
- Morning check-ins
- Focus suggestions
- Task completion celebrations
- Personalized messages based on personality

## Getting Started

### Prerequisites
- Node.js 18+
- npm or yarn
- Expo CLI
- Supabase account

### Installation

1. Clone the repository:
```bash
cd companion-ai
```

2. Install dependencies:
```bash
npm install
```

3. Set up Supabase:
   - Create a new Supabase project
   - Run the SQL schema from `supabase/schema.sql`
   - Copy your project URL and anon key

4. Configure environment:
Create a `.env` file:
```env
EXPO_PUBLIC_SUPABASE_URL=your_supabase_url
EXPO_PUBLIC_SUPABASE_ANON_KEY=your_supabase_anon_key
```

5. Start the development server:
```bash
npx expo start
```

## Project Structure

```
companion-ai/
‚îú‚îÄ‚îÄ App.tsx                 # Main app entry
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ companion/      # Companion avatar & chat
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks/          # Task cards & modals
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/             # Reusable UI components
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ voice/          # Voice input button
‚îÇ   ‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ supabase.ts     # Supabase client
‚îÇ   ‚îú‚îÄ‚îÄ navigation/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RootNavigator.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TabNavigator.tsx
‚îÇ   ‚îú‚îÄ‚îÄ screens/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AuthScreen.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CalendarScreen.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HomeScreen.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OnboardingScreen.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SettingsScreen.tsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ShopScreen.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TasksScreen.tsx
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ voiceService.ts # Voice intent parsing
‚îÇ   ‚îú‚îÄ‚îÄ stores/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ authStore.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ companionStore.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ taskStore.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ walletStore.ts
‚îÇ   ‚îú‚îÄ‚îÄ theme/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.ts        # Colors, spacing, typography
‚îÇ   ‚îî‚îÄ‚îÄ types/
‚îÇ       ‚îî‚îÄ‚îÄ index.ts        # TypeScript types
‚îî‚îÄ‚îÄ supabase/
    ‚îî‚îÄ‚îÄ schema.sql          # Database schema
```

## Voice Commands

### Adding Tasks
- "Add task to [task description]"
- "Remind me to [task description]"
- "I need to [task description] by [date/time]"

### Querying Tasks
- "What are my tasks for today?"
- "Show me my work tasks"
- "What didn't I finish last week?"

### Completing Tasks
- "Complete [task name]"
- "Mark [task name] as done"
- "I finished [task name]"

### Focus & Planning
- "What should I focus on today?"
- "Help me prioritize"
- "What's most important?"

## Tech Stack

- **Framework**: React Native + Expo
- **Navigation**: React Navigation
- **State Management**: Zustand
- **Backend**: Supabase (PostgreSQL + Auth)
- **Styling**: StyleSheet + expo-linear-gradient
- **Voice**: expo-av (recording) + external STT service

## Integrating Speech-to-Text

The app records audio using `expo-av`. To enable actual voice recognition, integrate with one of these services:

### Option 1: OpenAI Whisper API
```typescript
const transcribe = async (audioUri: string) => {
  const formData = new FormData();
  formData.append('file', {
    uri: audioUri,
    type: 'audio/m4a',
    name: 'audio.m4a',
  });
  formData.append('model', 'whisper-1');
  
  const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_API_KEY}`,
    },
    body: formData,
  });
  
  const { text } = await response.json();
  return text;
};
```

### Option 2: Google Cloud Speech-to-Text
### Option 3: AWS Transcribe
### Option 4: Azure Speech Services

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Open a Pull Request

## License

MIT License - feel free to use this for your own projects!

---

Built with ‚ù§Ô∏è for productivity and whimsy
